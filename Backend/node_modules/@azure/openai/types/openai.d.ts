import { AzureKeyCredential } from '@azure/core-auth';
import { ClientOptions } from '@azure-rest/core-client';
import { ErrorModel } from '@azure-rest/core-client';
import { KeyCredential } from '@azure/core-auth';
import { RawHttpHeadersInput } from '@azure/core-rest-pipeline';
import { TokenCredential } from '@azure/core-auth';

export { AzureKeyCredential }

/** The state of a job or item. */
/** "notRunning", "running", "succeeded", "canceled", "failed" */
export declare type AzureOpenAIOperationState = string;

/** A polling status update or final response payload for an image operation. */
export declare interface BatchImageGenerationOperationResponse {
    /** The ID of the operation. */
    id: string;
    /** A timestamp when this job or item was created (in unix epochs). */
    created: number;
    /** A timestamp when this operation and its associated images expire and will be deleted (in unix epochs). */
    expires?: number;
    /** The result of the operation if the operation succeeded. */
    result?: ImageGenerations;
    /** The status of the operation */
    status: AzureOpenAIOperationState;
    /** The error if the operation failed. */
    error?: ErrorModel;
}

export declare interface BeginAzureBatchImageGenerationOptions extends RequestOptions {
    /** The number of images to generate (defaults to 1). */
    n?: number;
    /** The desired size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 (defaults to 1024x1024). */
    size?: ImageSize;
    /**
     *   The format in which image generation response items should be presented.
     *   Azure OpenAI only supports URL response items.
     */
    responseFormat?: ImageGenerationResponseFormat;
    /** A unique identifier representing your end-user, which can help to monitor and detect abuse. */
    user?: string;
}

/**
 * The representation of a single prompt completion as part of an overall chat completions request.
 * Generally, `n` choices are generated per provided prompt with a default value of 1.
 * Token limits and other settings may limit the number of choices generated.
 */
export declare interface ChatChoice {
    /** The chat message for a given chat completions prompt. */
    message?: ChatMessage;
    /** The ordered index associated with this chat completions choice. */
    index: number;
    /** The reason that this chat completions choice completed its generated. */
    finishReason: CompletionsFinishReason | null;
    /** The delta message content for a streaming response. */
    delta?: ChatMessage;
}

/**
 * Representation of the response data from a chat completions request.
 * Completions support a wide variety of tasks and generate text that continues from or "completes"
 * provided prompt data.
 */
export declare interface ChatCompletions {
    /** A unique identifier associated with this chat completions response. */
    id: string;
    /**
     * The first timestamp associated with generation activity for this completions response,
     * represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
     */
    created: number;
    /**
     * The collection of completions choices associated with this completions response.
     * Generally, `n` choices are generated per provided prompt with a default value of 1.
     * Token limits and other settings may limit the number of choices generated.
     */
    choices: ChatChoice[];
    /** Usage information for tokens processed and generated as part of this completions operation. */
    usage: CompletionsUsage;
}

/** A single, role-attributed message within a chat completion interaction. */
export declare interface ChatMessage {
    /** The role associated with this message payload. */
    role: ChatRole;
    /** The text associated with this message payload. */
    content?: string;
}

/** A description of the intended purpose of a message within a chat completions interaction. */
/** "system", "assistant", "user" */
export declare type ChatRole = string;

/**
 * The representation of a single prompt completion as part of an overall completions request.
 * Generally, `n` choices are generated per provided prompt with a default value of 1.
 * Token limits and other settings may limit the number of choices generated.
 */
export declare interface Choice {
    /** The generated text for a given completions prompt. */
    text: string;
    /** The ordered index associated with this completions choice. */
    index: number;
    /** The log probabilities model for tokens associated with this completions choice. */
    logprobs: CompletionsLogProbabilityModel | null;
    /** Reason for finishing */
    finishReason: CompletionsFinishReason | null;
}

/**
 * Representation of the response data from a completions request.
 * Completions support a wide variety of tasks and generate text that continues from or "completes"
 * provided prompt data.
 */
export declare interface Completions {
    /** A unique identifier associated with this completions response. */
    id: string;
    /**
     * The first timestamp associated with generation activity for this completions response,
     * represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
     */
    created: number;
    /**
     * The collection of completions choices associated with this completions response.
     * Generally, `n` choices are generated per provided prompt with a default value of 1.
     * Token limits and other settings may limit the number of choices generated.
     */
    choices: Choice[];
    /** Usage information for tokens processed and generated as part of this completions operation. */
    usage: CompletionsUsage;
}

/** Representation of the manner in which a completions response concluded. */
/** "stop", "length", "content_filter" */
export declare type CompletionsFinishReason = string;

/** Representation of a log probabilities model for a completions generation. */
export declare interface CompletionsLogProbabilityModel {
    /** The textual forms of tokens evaluated in this probability model. */
    tokens: string[];
    /** A collection of log probability values for the tokens in this completions data. */
    tokenLogprobs: (number | null)[];
    /** A mapping of tokens to maximum log probability values in this completions data. */
    topLogprobs: Record<string, number | null>[];
    /** The text offsets associated with tokens in this completions data. */
    textOffset: number[];
}

/** Representation of a log probabilities model for a completions generation. */
export declare interface CompletionsLogProbabilityModel {
    /** The textual forms of tokens evaluated in this probability model. */
    tokens: string[];
    /** A collection of log probability values for the tokens in this completions data. */
    tokenLogprobs: (number | null)[];
    /** A mapping of tokens to maximum log probability values in this completions data. */
    topLogprobs: Record<string, number | null>[];
    /** The text offsets associated with tokens in this completions data. */
    textOffset: number[];
}

/**
 * Representation of the token counts processed for a completions request.
 * Counts consider all tokens across prompts, choices, choice alternates, best_of generations, and
 * other consumers.
 */
export declare interface CompletionsUsage {
    /** The number of tokens generated across all completions emissions. */
    completionTokens: number;
    /** The number of tokens in the provided prompts for the completions request. */
    promptTokens: number;
    /** The total number of tokens processed for the completions request and response. */
    totalTokens: number;
}

/** Representation of a single embeddings relatedness comparison. */
export declare interface EmbeddingItem {
    /**
     * List of embeddings value for the input prompt. These represent a measurement of the
     * vector-based relatedness of the provided input.
     */
    embedding: number[];
    /** Index of the prompt to which the EmbeddingItem corresponds. */
    index: number;
}

/**
 * Representation of the response data from an embeddings request.
 * Embeddings measure the relatedness of text strings and are commonly used for search, clustering,
 * recommendations, and other similar scenarios.
 */
export declare interface Embeddings {
    /** Embedding values for the prompts submitted in the request. */
    data: EmbeddingItem[];
    /** Usage counts for tokens input using the embeddings API. */
    usage: EmbeddingsUsage;
}

/** Measurement of the amount of tokens used in this request and response. */
export declare interface EmbeddingsUsage {
    /** Number of tokens sent in the original request. */
    promptTokens: number;
    /** Total number of tokens transacted in this request/response. */
    totalTokens: number;
}

export declare interface GetAzureBatchImageGenerationOperationStatusOptions extends RequestOptions {
}

export declare interface GetChatCompletionsOptions extends RequestOptions {
    /** The maximum number of tokens to generate. */
    maxTokens?: number;
    /**
     * The sampling temperature to use that controls the apparent creativity of generated completions.
     * Higher values will make output more random while lower values will make results more focused
     * and deterministic.
     * It is not recommended to modify temperature and top_p for the same completions request as the
     * interaction of these two settings is difficult to predict.
     */
    temperature?: number;
    /**
     * An alternative to sampling with temperature called nucleus sampling. This value causes the
     * model to consider the results of tokens with the provided probability mass. As an example, a
     * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
     * considered.
     * It is not recommended to modify temperature and top_p for the same completions request as the
     * interaction of these two settings is difficult to predict.
     */
    topP?: number;
    /**
     * A map between GPT token IDs and bias scores that influences the probability of specific tokens
     * appearing in a completions response. Token IDs are computed via external tokenizer tools, while
     * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
     * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
     * score varies by model.
     */
    logitBias?: Record<string, number>;
    /**
     * An identifier for the caller or end user of the operation. This may be used for tracking
     * or rate-limiting purposes.
     */
    user?: string;
    /**
     * The number of chat completions choices that should be generated for a chat completions
     * response.
     * Because this setting can generate many completions, it may quickly consume your token quota.
     * Use carefully and ensure reasonable settings for max_tokens and stop.
     */
    n?: number;
    /** A collection of textual sequences that will end completions generation. */
    stop?: string[];
    /**
     * A value that influences the probability of generated tokens appearing based on their existing
     * presence in generated text.
     * Positive values will make tokens less likely to appear when they already exist and increase the
     * model's likelihood to output new topics.
     */
    presencePenalty?: number;
    /**
     * A value that influences the probability of generated tokens appearing based on their cumulative
     * frequency in generated text.
     * Positive values will make tokens less likely to appear as their frequency increases and
     * decrease the likelihood of the model repeating the same statements verbatim.
     */
    frequencyPenalty?: number;
    /** A value indicating whether chat completions should be streamed for this request. */
    stream?: boolean;
    /**
     * The model name to provide as part of this completions request.
     * Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     * resource URI that's connected to.
     */
    model?: string;
}

export declare interface GetCompletionsOptions extends RequestOptions {
    /** The maximum number of tokens to generate. */
    maxTokens?: number;
    /**
     * The sampling temperature to use that controls the apparent creativity of generated completions.
     * Higher values will make output more random while lower values will make results more focused
     * and deterministic.
     * It is not recommended to modify temperature and top_p for the same completions request as the
     * interaction of these two settings is difficult to predict.
     */
    temperature?: number;
    /**
     * An alternative to sampling with temperature called nucleus sampling. This value causes the
     * model to consider the results of tokens with the provided probability mass. As an example, a
     * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
     * considered.
     * It is not recommended to modify temperature and top_p for the same completions request as the
     * interaction of these two settings is difficult to predict.
     */
    topP?: number;
    /**
     * A map between GPT token IDs and bias scores that influences the probability of specific tokens
     * appearing in a completions response. Token IDs are computed via external tokenizer tools, while
     * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
     * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
     * score varies by model.
     */
    logitBias?: Record<string, number>;
    /**
     * An identifier for the caller or end user of the operation. This may be used for tracking
     * or rate-limiting purposes.
     */
    user?: string;
    /**
     * The number of completions choices that should be generated per provided prompt as part of an
     * overall completions response.
     * Because this setting can generate many completions, it may quickly consume your token quota.
     * Use carefully and ensure reasonable settings for max_tokens and stop.
     */
    n?: number;
    /**
     * A value that controls the emission of log probabilities for the provided number of most likely
     * tokens within a completions response.
     */
    logprobs?: number;
    /**
     * A value specifying whether completions responses should include input prompts as prefixes to
     * their generated output.
     */
    echo?: boolean;
    /** A collection of textual sequences that will end completions generation. */
    stop?: string[];
    /**
     * A value that influences the probability of generated tokens appearing based on their existing
     * presence in generated text.
     * Positive values will make tokens less likely to appear when they already exist and increase the
     * model's likelihood to output new topics.
     */
    presencePenalty?: number;
    /**
     * A value that influences the probability of generated tokens appearing based on their cumulative
     * frequency in generated text.
     * Positive values will make tokens less likely to appear as their frequency increases and
     * decrease the likelihood of the model repeating the same statements verbatim.
     */
    frequencyPenalty?: number;
    /**
     * A value that controls how many completions will be internally generated prior to response
     * formulation.
     * When used together with n, best_of controls the number of candidate completions and must be
     * greater than n.
     * Because this setting can generate many completions, it may quickly consume your token quota.
     * Use carefully and ensure reasonable settings for max_tokens and stop.
     */
    bestOf?: number;
    /** A value indicating whether chat completions should be streamed for this request. */
    stream?: boolean;
    /**
     * The model name to provide as part of this completions request.
     * Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     * resource URI that's connected to.
     */
    model?: string;
}

export declare interface GetEmbeddingsOptions extends RequestOptions {
    /**
     * An identifier for the caller or end user of the operation. This may be used for tracking
     * or rate-limiting purposes.
     */
    user?: string;
    /**
     * The model name to provide as part of this embeddings request.
     * Not applicable to Azure OpenAI, where deployment information should be included in the Azure
     * resource URI that's connected to.
     */
    model?: string;
}

/** Convenience alias for BeginAzureBatchImageGenerationOptions */
export declare type ImageGenerationOptions = BeginAzureBatchImageGenerationOptions;

/** Convenience alias for BatchImageGenerationOperationResponse */
export declare type ImageGenerationResponse = BatchImageGenerationOperationResponse;

/** The format in which the generated images are returned. */
/** "url", "b64_json" */
export declare type ImageGenerationResponseFormat = string;

/** The result of the operation if the operation succeeded. */
export declare interface ImageGenerations {
    /** A timestamp when this job or item was created (in unix epochs). */
    created: number;
    /** The images generated by the operator. */
    data: ImageLocation[] | ImagePayload[];
}

/** An image response item that provides a URL from which an image may be accessed. */
export declare interface ImageLocation {
    /** The URL that provides temporary access to download the generated image. */
    url: string;
}

/** An image response item that directly represents the image data as a base64-encoded string. */
export declare interface ImagePayload {
    /** The complete data for an image represented as a base64-encoded string. */
    base64Data: string;
}

/** The desired size of the generated images. Must be one of 256x256, 512x512, or 1024x1024. */
/** "256x256", "512x512", "1024x1024" */
export declare type ImageSize = string;

export declare class OpenAIClient {
    private _client;
    private _isAzure;
    /**
     * Initializes an instance of OpenAIClient for use with an Azure OpenAI resource.
     * @param endpoint - The URI for an Azure OpenAI resource, including protocol and hostname.
     *                 For example: https://my-resource.openai.azure.com.
     * @param credential - A key credential used to authenticate to an Azure OpenAI resource.
     * @param options - The options for configuring the client.
     * @remarks
     *   This constructor initializes an OpenAIClient object that can only be used with Azure OpenAI resources.
     *   To use OpenAIClient with a non-Azure OpenAI inference endpoint, use a constructor that accepts a non-Azure OpenAI API key instead.
     */
    constructor(endpoint: string, credential: KeyCredential, options?: OpenAIClientOptions);
    /**
     * Initializes an instance of OpenAIClient for use with an Azure OpenAI resource.
     * @param endpoint - The URI for an Azure OpenAI resource, including protocol and hostname.
     *                 For example: https://my-resource.openai.azure.com.
     * @param credential - A token credential used to authenticate with an Azure OpenAI resource.
     * @param options - The options for configuring the client.
     */
    constructor(endpoint: string, credential: TokenCredential, options?: OpenAIClientOptions);
    /**
     * Initializes an instance of OpenAIClient for use with the non-Azure OpenAI endpoint.
     * @param openAiApiKey - The API key to use when connecting to the non-Azure OpenAI endpoint.
     * @param options - The options for configuring the client.
     * @remarks
     *   OpenAIClient objects initialized with this constructor can only be used with the non-Azure OpenAI inference endpoint.
     *   To use OpenAIClient with an Azure OpenAI resource, use a constructor that accepts a resource URI and Azure authentication credential instead.
     */
    constructor(openAiApiKey: KeyCredential, options?: OpenAIClientOptions);
    /** Returns the status of the images operation */
    getAzureBatchImageGenerationOperationStatus(operationId: string, options?: GetAzureBatchImageGenerationOperationStatusOptions): Promise<ImageGenerationResponse>;
    /** Starts the generation of a batch of images from a text caption */
    beginAzureBatchImageGeneration(prompt: string, options?: ImageGenerationOptions): Promise<ImageGenerationResponse>;
    /**
     * Starts the generation of a batch of images from a text caption
     * @param prompt - The prompt to use for this request.
     * @param options - The options for this image request.
     * @returns The image generation response (containing url or base64 data).
     */
    getImages(prompt: string, options?: ImageGenerationOptions): Promise<ImageGenerationResponse>;
    /**
     * Returns textual completions as configured for a given prompt.
     * @param deploymentOrModelName - Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request.
     * @param prompt - The prompt to use for this request.
     * @param options - The options for this completions request.
     * @returns The completions for the given prompt.
     */
    getCompletions(deploymentOrModelName: string, prompt: string[], options?: GetCompletionsOptions): Promise<Completions>;
    /**
     * Lists the completions tokens as they become available for a given prompt.
     * @param deploymentOrModelName - The name of the model deployment (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request.
     * @param prompt - The prompt to use for this request.
     * @param options - The completions options for this completions request.
     * @returns An asynchronous iterable of completions tokens.
     */
    listCompletions(deploymentOrModelName: string, prompt: string[], options?: GetCompletionsOptions): Promise<AsyncIterable<Omit<Completions, "usage">>>;
    /**
     * Return the computed embeddings for a given prompt.
     * @param deploymentOrModelName - The name of the model deployment (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request.
     * @param input - The prompt to use for this request.
     * @param options - The embeddings options for this embeddings request.
     * @returns The embeddings for the given prompt.
     */
    getEmbeddings(deploymentOrModelName: string, input: string[], options?: GetEmbeddingsOptions): Promise<Embeddings>;
    /**
     * Get chat completions for provided chat context messages.
     * @param deploymentOrModelName - The name of the model deployment (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request.
     * @param messages - The chat context messages to use for this request.
     * @param options - The chat completions options for this completions request.
     * @returns The chat completions for the given chat context messages.
     */
    getChatCompletions(deploymentOrModelName: string, messages: ChatMessage[], options?: GetChatCompletionsOptions): Promise<ChatCompletions>;
    /**
     * Lists the chat completions tokens as they become available for a chat context.
     * @param deploymentOrModelName - The name of the model deployment (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request.
     * @param messages - The chat context messages to use for this request.
     * @param options - The chat completions options for this chat completions request.
     * @returns An asynchronous iterable of chat completions tokens.
     */
    listChatCompletions(deploymentOrModelName: string, messages: ChatMessage[], options?: GetChatCompletionsOptions): Promise<AsyncIterable<Omit<ChatCompletions, "usage">>>;
    private setModel;
}

export declare interface OpenAIClientOptions extends ClientOptions {
}

/**
 * The OpenAIKeyCredential class represents an OpenAI API key
 * and is used to authenticate into an OpenAI client for
 * an OpenAI endpoint.
 */
export declare class OpenAIKeyCredential implements KeyCredential {
    private _key;
    /**
     * Create an instance of an AzureKeyCredential for use
     * with a service client.
     *
     * @param key - The initial value of the key to use in authentication
     */
    constructor(key: string);
    /**
     * The value of the key to be used in authentication
     */
    get key(): string;
    /**
     * Change the value of the key.
     *
     * Updates will take effect upon the next request after
     * updating the key value.
     *
     * @param newKey - The new key value to be used
     */
    update(newKey: string): void;
}

/**
 * Common options to set on an outgoing operation
 */
export declare interface RequestOptions {
    /**
     * Options to set on an outgoing HTTP request
     */
    requestOptions?: {
        /**
         * Headers to send along with the request
         */
        headers?: RawHttpHeadersInput;
        /** Set to true if the request is sent over HTTP instead of HTTPS */
        allowInsecureConnection?: boolean;
        /** Set to true if you want to skip encoding the path parameters */
        skipUrlEncoding?: boolean;
    };
}

export { }
