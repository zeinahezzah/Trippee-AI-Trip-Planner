{"version":3,"file":"operations.js","sourceRoot":"","sources":["../../../src/api/operations.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC;AAYlC,OAAO,EAeL,YAAY,GACb,MAAM,kBAAkB,CAAC;AAqM1B,MAAM,UAAU,kBAAkB,CAChC,OAAe,EACf,KAAe,EACf,YAAoB,EACpB,UAAgC,EAAE,cAAc,EAAE,EAAE,EAAE;;IAEtD,OAAO,OAAO,CAAC,IAAI,CAAC,wCAAwC,EAAE,YAAY,CAAC,CAAC,IAAI,CAAC;QAC/E,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE,EAAE,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI,EAAE,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,KAAK,EAAE,KAAK,EAAE,KAAK,EAAE;KACnE,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,yBAAyB,CAC7C,MAA+D;;IAE/D,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,IAAI,EAAE,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YAC5C,SAAS,EAAE,CAAC,CAAC,WAAW,CAAC;YACzB,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;SAClB,CAAC,CAAC;QACH,KAAK,EAAE;YACL,YAAY,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;YAChD,WAAW,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;SAC/C;KACF,CAAC;AACJ,CAAC;AAED,gDAAgD;AAChD,MAAM,CAAC,KAAK,UAAU,aAAa,CACjC,OAAe,EACf,KAAe,EACf,YAAoB,EACpB,UAAgC,EAAE,cAAc,EAAE,EAAE,EAAE;IAEtD,MAAM,MAAM,GAAG,MAAM,kBAAkB,CAAC,OAAO,EAAE,KAAK,EAAE,YAAY,EAAE,OAAO,CAAC,CAAC;IAC/E,OAAO,yBAAyB,CAAC,MAAM,CAAC,CAAC;AAC3C,CAAC;AAED,MAAM,UAAU,mBAAmB,CACjC,OAAe,EACf,MAAgB,EAChB,YAAoB,EACpB,UAAiC,EAAE,cAAc,EAAE,EAAE,EAAE;;IAEvD,OAAO,OAAO,CAAC,IAAI,CAAC,yCAAyC,EAAE,YAAY,CAAC,CAAC,IAAI,CAAC;QAChF,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE;YACJ,MAAM,EAAE,MAAM;YACd,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,WAAW,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,WAAW;YACjC,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACpB,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,CAAC,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,CAAC;YACb,QAAQ,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,QAAQ;YAC3B,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,gBAAgB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,eAAe;YAC1C,iBAAiB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,gBAAgB;YAC5C,OAAO,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,MAAM;YACxB,MAAM,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,MAAM;YACvB,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,KAAK;SACtB;KACF,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,0BAA0B,CAC9C,MAAiE;;IAEjE,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,EAAE,EAAE,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;QACrB,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,OAAO,EAAE,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YAClD,IAAI,EAAE,CAAC,CAAC,MAAM,CAAC;YACf,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;YACjB,QAAQ,EACN,CAAC,CAAC,QAAQ,KAAK,IAAI;gBACjB,CAAC,CAAC,IAAI;gBACN,CAAC,CAAC;oBACE,MAAM,EAAE,CAAC,CAAC,QAAQ,CAAC,QAAQ,CAAC;oBAC5B,aAAa,EAAE,CAAC,CAAC,QAAQ,CAAC,gBAAgB,CAAC;oBAC3C,WAAW,EAAE,CAAC,CAAC,QAAQ,CAAC,cAAc,CAAC;oBACvC,UAAU,EAAE,CAAC,CAAC,QAAQ,CAAC,aAAa,CAAC;iBACtC;YACP,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;SACjC,CAAC,CAAC;QACH,KAAK,EAAE;YACL,gBAAgB,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,mBAAmB,CAAC;YACxD,YAAY,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;YAChD,WAAW,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;SAC/C;KACF,CAAC;AACJ,CAAC;AAED;;;;GAIG;AACH,MAAM,CAAC,KAAK,UAAU,cAAc,CAClC,OAAe,EACf,MAAgB,EAChB,YAAoB,EACpB,UAAiC,EAAE,cAAc,EAAE,EAAE,EAAE;IAEvD,MAAM,MAAM,GAAG,MAAM,mBAAmB,CAAC,OAAO,EAAE,MAAM,EAAE,YAAY,EAAE,OAAO,CAAC,CAAC;IACjF,OAAO,0BAA0B,CAAC,MAAM,CAAC,CAAC;AAC5C,CAAC;AAED,MAAM,UAAU,uBAAuB,CACrC,OAAe,EACf,QAAuB,EACvB,YAAoB,EACpB,UAAqC,EAAE,cAAc,EAAE,EAAE,EAAE;;IAE3D,OAAO,OAAO,CAAC,IAAI,CAAC,8CAA8C,EAAE,YAAY,CAAC,CAAC,IAAI,CAAC;QACrF,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE;YACJ,QAAQ,EAAE,QAAQ;YAClB,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,WAAW,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,WAAW;YACjC,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACpB,UAAU,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,SAAS;YAC9B,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,CAAC,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,CAAC;YACb,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,gBAAgB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,eAAe;YAC1C,iBAAiB,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,gBAAgB;YAC5C,MAAM,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,MAAM;YACvB,KAAK,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,KAAK;SACtB;KACF,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,8BAA8B,CAClD,MAAyE;;IAEzE,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,EAAE,EAAE,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;QACrB,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,OAAO,EAAE,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE;;YAAC,OAAA,CAAC;gBAClD,OAAO,EAAE,CAAC,CAAC,CAAC,OAAO;oBACjB,CAAC,CAAC,SAAS;oBACX,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,SAAS,CAAC,EAAE;gBAClE,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;gBACjB,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;gBAChC,KAAK,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,SAAS,CAAC,EAAE;aACzF,CAAC,CAAA;SAAA,CAAC;QACH,KAAK,EAAE;YACL,gBAAgB,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,mBAAmB,CAAC;YACxD,YAAY,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC;YAChD,WAAW,EAAE,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,cAAc,CAAC;SAC/C;KACF,CAAC;AACJ,CAAC;AAED;;;;GAIG;AACH,MAAM,CAAC,KAAK,UAAU,kBAAkB,CACtC,OAAe,EACf,QAAuB,EACvB,YAAoB,EACpB,UAAqC,EAAE,cAAc,EAAE,EAAE,EAAE;IAE3D,MAAM,MAAM,GAAG,MAAM,uBAAuB,CAAC,OAAO,EAAE,QAAQ,EAAE,YAAY,EAAE,OAAO,CAAC,CAAC;IACvF,OAAO,8BAA8B,CAAC,MAAM,CAAC,CAAC;AAChD,CAAC;AAED,MAAM,UAAU,gDAAgD,CAC9D,OAAe,EACf,WAAmB,EACnB,UAA8D;IAC5D,cAAc,EAAE,EAAE;CACnB;;IAKD,OAAO,OAAO,CAAC,IAAI,CAAC,kCAAkC,EAAE,WAAW,CAAC,CAAC,GAAG,CAAC;QACvE,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;KAChD,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,uDAAuD,CAC3E,MAE8D;;IAE9D,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,EAAE,EAAE,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;QACrB,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,MAAM,EAAE,CAAC,MAAM,CAAC,IAAI,CAAC,MAAM;YACzB,CAAC,CAAC,SAAS;YACX,CAAC,CAAC;gBACE,OAAO,EAAE,MAAA,MAAM,CAAC,IAAI,CAAC,MAAM,0CAAG,SAAS,CAAC;gBACxC,IAAI,EAAE,sBAAsB,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,MAAM,0CAAG,MAAM,CAAC,CAAC;aAC3D;QACL,MAAM,EAAE,MAAM,CAAC,IAAI,CAAC,QAAQ,CAAC;QAC7B,KAAK,EAAE,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK;KAC1D,CAAC;AACJ,CAAC;AAED;;;;GAIG;AACH,SAAS,sBAAsB,CAC7B,KAAqC;IAErC,OAAO,KAAK,CAAC,GAAG,CAAC,UAAU,iBAAiB;QAC1C,IAAI,KAAK,IAAI,iBAAiB,EAAE;YAC9B,OAAO,iBAAkC,CAAC;SAC3C;aAAM;YACL,OAAO;gBACL,UAAU,EAAE,iBAAiB,CAAC,QAAQ;aACvB,CAAC;SACnB;IACH,CAAC,CAAqC,CAAC;AACzC,CAAC;AAED,iDAAiD;AACjD,MAAM,CAAC,KAAK,UAAU,2CAA2C,CAC/D,OAAe,EACf,WAAmB,EACnB,UAA8D;IAC5D,cAAc,EAAE,EAAE;CACnB;IAED,MAAM,MAAM,GAAG,MAAM,gDAAgD,CACnE,OAAO,EACP,WAAW,EACX,OAAO,CACR,CAAC;IACF,OAAO,uDAAuD,CAAC,MAAM,CAAC,CAAC;AACzE,CAAC;AAED,MAAM,UAAU,mCAAmC,CACjD,OAAe,EACf,MAAc,EACd,UAAiD,EAAE,cAAc,EAAE,EAAE,EAAE;;IAIvE,OAAO,OAAO,CAAC,IAAI,CAAC,4BAA4B,CAAC,CAAC,IAAI,CAAC;QACrD,uBAAuB,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,uBAAuB;QACxE,eAAe,EAAE,MAAA,OAAO,CAAC,cAAc,0CAAE,eAAe;QACxD,OAAO,oBAAO,MAAA,OAAO,CAAC,cAAc,0CAAE,OAAO,CAAE;QAC/C,IAAI,EAAE;YACJ,MAAM,EAAE,MAAM;YACd,CAAC,EAAE,MAAA,OAAO,CAAC,CAAC,mCAAI,CAAC;YACjB,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;YACnB,eAAe,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,cAAc;YACxC,IAAI,EAAE,OAAO,aAAP,OAAO,uBAAP,OAAO,CAAE,IAAI;SACpB;KACF,CAAC,CAAC;AACL,CAAC;AAED,MAAM,CAAC,KAAK,UAAU,0CAA0C,CAC9D,MAAiG;;IAEjG,IAAI,YAAY,CAAC,MAAM,CAAC,EAAE;QACxB,MAAM,MAAM,CAAC,IAAI,CAAC;KACnB;IAED,OAAO;QACL,EAAE,EAAE,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC;QACrB,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,OAAO,EAAE,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC;QAC/B,MAAM,EAAE,CAAC,MAAM,CAAC,IAAI,CAAC,MAAM;YACzB,CAAC,CAAC,SAAS;YACX,CAAC,CAAC;gBACE,OAAO,EAAE,MAAA,MAAM,CAAC,IAAI,CAAC,MAAM,0CAAG,SAAS,CAAC;gBACxC,IAAI,EAAE,sBAAsB,CAAC,MAAA,MAAM,CAAC,IAAI,CAAC,MAAM,0CAAG,MAAM,CAAC,CAAC;aAC3D;QACL,MAAM,EAAE,MAAM,CAAC,IAAI,CAAC,QAAQ,CAAC;QAC7B,KAAK,EAAE,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,MAAM,CAAC,IAAI,CAAC,KAAK;KAC1D,CAAC;AACJ,CAAC;AAED,qEAAqE;AACrE,MAAM,CAAC,KAAK,UAAU,8BAA8B,CAClD,OAAe,EACf,MAAc,EACd,UAAiD,EAAE,cAAc,EAAE,EAAE,EAAE;IAEvE,MAAM,MAAM,GAAG,MAAM,mCAAmC,CAAC,OAAO,EAAE,MAAM,EAAE,OAAO,CAAC,CAAC;IACnF,OAAO,0CAA0C,CAAC,MAAM,CAAC,CAAC;AAC5D,CAAC;AAED,MAAM,UAAU,oBAAoB,CAAC,IAAyB;;IAC5D,OAAO;QACL,EAAE,EAAE,IAAI,CAAC,IAAI,CAAC;QACd,OAAO,EAAE,IAAI,CAAC,SAAS,CAAC;QACxB,OAAO,EAAE,CAAC,MAAA,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAe,EAAE,EAAE,CAAC,CAAC;YACzD,IAAI,EAAE,CAAC,CAAC,MAAM,CAAC;YACf,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;YACjB,QAAQ,EACN,CAAC,CAAC,QAAQ,KAAK,IAAI;gBACjB,CAAC,CAAC,IAAI;gBACN,CAAC,CAAC;oBACE,MAAM,EAAE,CAAC,CAAC,QAAQ,CAAC,QAAQ,CAAC;oBAC5B,aAAa,EAAE,CAAC,CAAC,QAAQ,CAAC,gBAAgB,CAAC;oBAC3C,WAAW,EAAE,CAAC,CAAC,QAAQ,CAAC,cAAc,CAAC;oBACvC,UAAU,EAAE,CAAC,CAAC,QAAQ,CAAC,aAAa,CAAC;iBACtC;YACP,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;SACjC,CAAC,CAAC;KACJ,CAAC;AACJ,CAAC;AAED,MAAM,UAAU,wBAAwB,CACtC,IAAyB;;IAEzB,OAAO;QACL,EAAE,EAAE,IAAI,CAAC,IAAI,CAAC;QACd,OAAO,EAAE,IAAI,CAAC,SAAS,CAAC;QACxB,OAAO,EAAE,CAAC,MAAA,IAAI,CAAC,SAAS,CAAC,mCAAI,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAmB,EAAE,EAAE;;YAAC,OAAA,CAAC;gBAC7D,OAAO,EAAE,CAAC,CAAC,CAAC,OAAO;oBACjB,CAAC,CAAC,SAAS;oBACX,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,OAAO,0CAAG,SAAS,CAAC,EAAE;gBAClE,KAAK,EAAE,CAAC,CAAC,OAAO,CAAC;gBACjB,YAAY,EAAE,CAAC,CAAC,eAAe,CAAC;gBAChC,KAAK,EAAE,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,MAAM,CAAC,EAAE,OAAO,EAAE,MAAA,CAAC,CAAC,KAAK,0CAAG,SAAS,CAAC,EAAE;aACzF,CAAC,CAAA;SAAA,CAAC;KACJ,CAAC;AACJ,CAAC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\n/**\n * THIS IS AN AUTO-GENERATED FILE - DO NOT EDIT!\n *\n * Any changes you make here may be lost.\n *\n * If you need to make changes, please do so in the original source file, \\{project-root\\}/sources/custom\n */\n\nimport { StreamableMethod } from \"@azure-rest/core-client\";\nimport { RequestOptions } from \"../common/interfaces.js\";\nimport {\n  BeginAzureBatchImageGeneration202Response,\n  BeginAzureBatchImageGenerationDefaultResponse,\n  ChatChoiceOutput,\n  ChoiceOutput,\n  OpenAIContext as Client,\n  GetAzureBatchImageGenerationOperationStatus200Response,\n  GetAzureBatchImageGenerationOperationStatusDefaultResponse,\n  GetChatCompletions200Response,\n  GetChatCompletionsDefaultResponse,\n  GetCompletions200Response,\n  GetCompletionsDefaultResponse,\n  GetEmbeddings200Response,\n  GetEmbeddingsDefaultResponse,\n  ImageGenerationsOutput,\n  isUnexpected,\n} from \"../rest/index.js\";\nimport {\n  BatchImageGenerationOperationResponse,\n  ChatCompletions,\n  ChatMessage,\n  Completions,\n  Embeddings,\n  ImageGenerationResponseFormat,\n  ImageLocation,\n  ImagePayload,\n  ImageSize,\n} from \"./models.js\";\n\nexport interface GetEmbeddingsOptions extends RequestOptions {\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The model name to provide as part of this embeddings request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\nexport interface GetCompletionsOptions extends RequestOptions {\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of completions choices that should be generated per provided prompt as part of an\n   * overall completions response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /**\n   * A value that controls the emission of log probabilities for the provided number of most likely\n   * tokens within a completions response.\n   */\n  logprobs?: number;\n  /**\n   * A value specifying whether completions responses should include input prompts as prefixes to\n   * their generated output.\n   */\n  echo?: boolean;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /**\n   * A value that controls how many completions will be internally generated prior to response\n   * formulation.\n   * When used together with n, best_of controls the number of candidate completions and must be\n   * greater than n.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  bestOf?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\nexport interface GetChatCompletionsOptions extends RequestOptions {\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of chat completions choices that should be generated for a chat completions\n   * response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\nexport interface GetAzureBatchImageGenerationOperationStatusOptions extends RequestOptions {}\n\nexport interface BeginAzureBatchImageGenerationOptions extends RequestOptions {\n  /** The number of images to generate (defaults to 1). */\n  n?: number;\n  /** The desired size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 (defaults to 1024x1024). */\n  size?: ImageSize;\n  /**\n   *   The format in which image generation response items should be presented.\n   *   Azure OpenAI only supports URL response items.\n   */\n  responseFormat?: ImageGenerationResponseFormat;\n  /** A unique identifier representing your end-user, which can help to monitor and detect abuse. */\n  user?: string;\n}\n\n/** Convenience alias for BeginAzureBatchImageGenerationOptions */\nexport type ImageGenerationOptions = BeginAzureBatchImageGenerationOptions;\n\nexport function _getEmbeddingsSend(\n  context: Client,\n  input: string[],\n  deploymentId: string,\n  options: GetEmbeddingsOptions = { requestOptions: {} }\n): StreamableMethod<GetEmbeddings200Response | GetEmbeddingsDefaultResponse> {\n  return context.path(\"/deployments/{deploymentId}/embeddings\", deploymentId).post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: { user: options?.user, model: options?.model, input: input },\n  });\n}\n\nexport async function _getEmbeddingsDeserialize(\n  result: GetEmbeddings200Response | GetEmbeddingsDefaultResponse\n): Promise<Embeddings> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    data: (result.body[\"data\"] ?? []).map((p) => ({\n      embedding: p[\"embedding\"],\n      index: p[\"index\"],\n    })),\n    usage: {\n      promptTokens: result.body.usage[\"prompt_tokens\"],\n      totalTokens: result.body.usage[\"total_tokens\"],\n    },\n  };\n}\n\n/** Return the embeddings for a given prompt. */\nexport async function getEmbeddings(\n  context: Client,\n  input: string[],\n  deploymentId: string,\n  options: GetEmbeddingsOptions = { requestOptions: {} }\n): Promise<Embeddings> {\n  const result = await _getEmbeddingsSend(context, input, deploymentId, options);\n  return _getEmbeddingsDeserialize(result);\n}\n\nexport function _getCompletionsSend(\n  context: Client,\n  prompt: string[],\n  deploymentId: string,\n  options: GetCompletionsOptions = { requestOptions: {} }\n): StreamableMethod<GetCompletions200Response | GetCompletionsDefaultResponse> {\n  return context.path(\"/deployments/{deploymentId}/completions\", deploymentId).post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: {\n      prompt: prompt,\n      max_tokens: options?.maxTokens,\n      temperature: options?.temperature,\n      top_p: options?.topP,\n      logit_bias: options?.logitBias,\n      user: options?.user,\n      n: options?.n,\n      logprobs: options?.logprobs,\n      echo: options?.echo,\n      stop: options?.stop,\n      presence_penalty: options?.presencePenalty,\n      frequency_penalty: options?.frequencyPenalty,\n      best_of: options?.bestOf,\n      stream: options?.stream,\n      model: options?.model,\n    },\n  });\n}\n\nexport async function _getCompletionsDeserialize(\n  result: GetCompletions200Response | GetCompletionsDefaultResponse\n): Promise<Completions> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    id: result.body[\"id\"],\n    created: result.body[\"created\"],\n    choices: (result.body[\"choices\"] ?? []).map((p) => ({\n      text: p[\"text\"],\n      index: p[\"index\"],\n      logprobs:\n        p.logprobs === null\n          ? null\n          : {\n              tokens: p.logprobs[\"tokens\"],\n              tokenLogprobs: p.logprobs[\"token_logprobs\"],\n              topLogprobs: p.logprobs[\"top_logprobs\"],\n              textOffset: p.logprobs[\"text_offset\"],\n            },\n      finishReason: p[\"finish_reason\"],\n    })),\n    usage: {\n      completionTokens: result.body.usage[\"completion_tokens\"],\n      promptTokens: result.body.usage[\"prompt_tokens\"],\n      totalTokens: result.body.usage[\"total_tokens\"],\n    },\n  };\n}\n\n/**\n * Gets completions for the provided input prompts.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport async function getCompletions(\n  context: Client,\n  prompt: string[],\n  deploymentId: string,\n  options: GetCompletionsOptions = { requestOptions: {} }\n): Promise<Completions> {\n  const result = await _getCompletionsSend(context, prompt, deploymentId, options);\n  return _getCompletionsDeserialize(result);\n}\n\nexport function _getChatCompletionsSend(\n  context: Client,\n  messages: ChatMessage[],\n  deploymentId: string,\n  options: GetChatCompletionsOptions = { requestOptions: {} }\n): StreamableMethod<GetChatCompletions200Response | GetChatCompletionsDefaultResponse> {\n  return context.path(\"/deployments/{deploymentId}/chat/completions\", deploymentId).post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: {\n      messages: messages,\n      max_tokens: options?.maxTokens,\n      temperature: options?.temperature,\n      top_p: options?.topP,\n      logit_bias: options?.logitBias,\n      user: options?.user,\n      n: options?.n,\n      stop: options?.stop,\n      presence_penalty: options?.presencePenalty,\n      frequency_penalty: options?.frequencyPenalty,\n      stream: options?.stream,\n      model: options?.model,\n    },\n  });\n}\n\nexport async function _getChatCompletionsDeserialize(\n  result: GetChatCompletions200Response | GetChatCompletionsDefaultResponse\n): Promise<ChatCompletions> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    id: result.body[\"id\"],\n    created: result.body[\"created\"],\n    choices: (result.body[\"choices\"] ?? []).map((p) => ({\n      message: !p.message\n        ? undefined\n        : { role: p.message?.[\"role\"], content: p.message?.[\"content\"] },\n      index: p[\"index\"],\n      finishReason: p[\"finish_reason\"],\n      delta: !p.delta ? undefined : { role: p.delta?.[\"role\"], content: p.delta?.[\"content\"] },\n    })),\n    usage: {\n      completionTokens: result.body.usage[\"completion_tokens\"],\n      promptTokens: result.body.usage[\"prompt_tokens\"],\n      totalTokens: result.body.usage[\"total_tokens\"],\n    },\n  };\n}\n\n/**\n * Gets chat completions for the provided chat messages.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport async function getChatCompletions(\n  context: Client,\n  messages: ChatMessage[],\n  deploymentId: string,\n  options: GetChatCompletionsOptions = { requestOptions: {} }\n): Promise<ChatCompletions> {\n  const result = await _getChatCompletionsSend(context, messages, deploymentId, options);\n  return _getChatCompletionsDeserialize(result);\n}\n\nexport function _getAzureBatchImageGenerationOperationStatusSend(\n  context: Client,\n  operationId: string,\n  options: GetAzureBatchImageGenerationOperationStatusOptions = {\n    requestOptions: {},\n  }\n): StreamableMethod<\n  | GetAzureBatchImageGenerationOperationStatus200Response\n  | GetAzureBatchImageGenerationOperationStatusDefaultResponse\n> {\n  return context.path(\"/operations/images/{operationId}\", operationId).get({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n  });\n}\n\nexport async function _getAzureBatchImageGenerationOperationStatusDeserialize(\n  result:\n    | GetAzureBatchImageGenerationOperationStatus200Response\n    | GetAzureBatchImageGenerationOperationStatusDefaultResponse\n): Promise<BatchImageGenerationOperationResponse> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    id: result.body[\"id\"],\n    created: result.body[\"created\"],\n    expires: result.body[\"expires\"],\n    result: !result.body.result\n      ? undefined\n      : {\n          created: result.body.result?.[\"created\"],\n          data: toImageGenerationsData(result.body.result?.[\"data\"]),\n        },\n    status: result.body[\"status\"],\n    error: !result.body.error ? undefined : result.body.error,\n  };\n}\n\n/**\n * Convert REST-level ImageGenerationsOutput.data to HLC-level ImageGenerations.data\n * see https://github.com/Azure/autorest.typescript/issues/1921\n * @internal\n */\nfunction toImageGenerationsData(\n  input: ImageGenerationsOutput[\"data\"]\n): ImageLocation[] | ImagePayload[] {\n  return input.map(function (locationOrPayload) {\n    if (\"url\" in locationOrPayload) {\n      return locationOrPayload as ImageLocation;\n    } else {\n      return {\n        base64Data: locationOrPayload.b64_json,\n      } as ImagePayload;\n    }\n  }) as ImageLocation[] | ImagePayload[];\n}\n\n/** Returns the status of the images operation */\nexport async function getAzureBatchImageGenerationOperationStatus(\n  context: Client,\n  operationId: string,\n  options: GetAzureBatchImageGenerationOperationStatusOptions = {\n    requestOptions: {},\n  }\n): Promise<BatchImageGenerationOperationResponse> {\n  const result = await _getAzureBatchImageGenerationOperationStatusSend(\n    context,\n    operationId,\n    options\n  );\n  return _getAzureBatchImageGenerationOperationStatusDeserialize(result);\n}\n\nexport function _beginAzureBatchImageGenerationSend(\n  context: Client,\n  prompt: string,\n  options: BeginAzureBatchImageGenerationOptions = { requestOptions: {} }\n): StreamableMethod<\n  BeginAzureBatchImageGeneration202Response | BeginAzureBatchImageGenerationDefaultResponse\n> {\n  return context.path(\"/images/generations:submit\").post({\n    allowInsecureConnection: options.requestOptions?.allowInsecureConnection,\n    skipUrlEncoding: options.requestOptions?.skipUrlEncoding,\n    headers: { ...options.requestOptions?.headers },\n    body: {\n      prompt: prompt,\n      n: options.n ?? 1,\n      size: options?.size,\n      response_format: options?.responseFormat,\n      user: options?.user,\n    },\n  });\n}\n\nexport async function _beginAzureBatchImageGenerationDeserialize(\n  result: BeginAzureBatchImageGeneration202Response | BeginAzureBatchImageGenerationDefaultResponse\n): Promise<BatchImageGenerationOperationResponse> {\n  if (isUnexpected(result)) {\n    throw result.body;\n  }\n\n  return {\n    id: result.body[\"id\"],\n    created: result.body[\"created\"],\n    expires: result.body[\"expires\"],\n    result: !result.body.result\n      ? undefined\n      : {\n          created: result.body.result?.[\"created\"],\n          data: toImageGenerationsData(result.body.result?.[\"data\"]),\n        },\n    status: result.body[\"status\"],\n    error: !result.body.error ? undefined : result.body.error,\n  };\n}\n\n/** Starts the generation of a batch of images from a text caption */\nexport async function beginAzureBatchImageGeneration(\n  context: Client,\n  prompt: string,\n  options: BeginAzureBatchImageGenerationOptions = { requestOptions: {} }\n): Promise<BatchImageGenerationOperationResponse> {\n  const result = await _beginAzureBatchImageGenerationSend(context, prompt, options);\n  return _beginAzureBatchImageGenerationDeserialize(result);\n}\n\nexport function getCompletionsResult(body: Record<string, any>): Omit<Completions, \"usage\"> {\n  return {\n    id: body[\"id\"],\n    created: body[\"created\"],\n    choices: (body[\"choices\"] ?? []).map((p: ChoiceOutput) => ({\n      text: p[\"text\"],\n      index: p[\"index\"],\n      logprobs:\n        p.logprobs === null\n          ? null\n          : {\n              tokens: p.logprobs[\"tokens\"],\n              tokenLogprobs: p.logprobs[\"token_logprobs\"],\n              topLogprobs: p.logprobs[\"top_logprobs\"],\n              textOffset: p.logprobs[\"text_offset\"],\n            },\n      finishReason: p[\"finish_reason\"],\n    })),\n  };\n}\n\nexport function getChatCompletionsResult(\n  body: Record<string, any>\n): Omit<ChatCompletions, \"usage\"> {\n  return {\n    id: body[\"id\"],\n    created: body[\"created\"],\n    choices: (body[\"choices\"] ?? []).map((p: ChatChoiceOutput) => ({\n      message: !p.message\n        ? undefined\n        : { role: p.message?.[\"role\"], content: p.message?.[\"content\"] },\n      index: p[\"index\"],\n      finishReason: p[\"finish_reason\"],\n      delta: !p.delta ? undefined : { role: p.delta?.[\"role\"], content: p.delta?.[\"content\"] },\n    })),\n  };\n}\n"]}